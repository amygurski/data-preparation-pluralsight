{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7838e31b-028a-49e6-a344-cb60d1f13313",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# os.environ[\"AZURE_STORAGE_KEY\"] = <key>\n",
    "\n",
    "# Should use unity catalog but was struggling with that, need to come back to this.\n",
    "spark.conf.set(\n",
    "    \"fs.azure.account.key.ughnovgs.dfs.core.windows.net\",\n",
    "    os.environ[\"AZURE_STORAGE_KEY\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b822cdf-33a4-4473-a143-1ce55a0911e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Replace with your values\n",
    "storage_account_name = \"ughnovgs\"\n",
    "container_name = \"raw\"\n",
    "\n",
    "display(dbutils.fs.ls(\n",
    "    f\"abfss://{container_name}@{storage_account_name}.dfs.core.windows.net/\",\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97689fa5-f8e7-4211-a24d-8bfa08a98ae7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks data profile. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\nif hasattr(dbutils, \"data\") and hasattr(dbutils.data, \"summarize\"):\n  # setup\n  __data_summary_display_orig = display\n  __data_summary_dfs = []\n  def __data_summary_display_new(df):\n    # add only when result is going to be table type\n    __data_summary_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\", \"pyspark.sql.classic.dataframe\"]\n    if (type(df).__module__ in __data_summary_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n      __data_summary_dfs.append(df)\n  display = __data_summary_display_new\n\n  def __data_summary_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"eWVsbG93VGF4aUZpbGVQYXRoID0gImFiZnNzOi8vcmF3QHVnaG5vdmdzLmRmcy5jb3JlLndpbmRvd3MubmV0L1llbGxvd1RheGlzXzIwMjUwMS5jc3YiCgp5ZWxsb3dUYXhpREYgPSAoCiAgICBzcGFyay5yZWFkCiAgICAub3B0aW9uKCJoZWFkZXIiLCAidHJ1ZSIpCiAgICAub3B0aW9uKCJpbmZlclNjaGVtYSIsICJ0cnVlIikKICAgIC5jc3YoeWVsbG93VGF4aUZpbGVQYXRoKQopCgp5ZWxsb3dUYXhpREYucHJpbnRTY2hlbWEoKQoKZGlzcGxheSh5ZWxsb3dUYXhpREYp\").decode())\n\n  try:\n    # run user code\n    __data_summary_user_code_fn()\n\n    # run on valid tableResultIndex\n    if len(__data_summary_dfs) > 0:\n      # run summarize\n      if type(__data_summary_dfs[0]).__module__ == \"databricks.koalas.frame\":\n        # koalas dataframe\n        dbutils.data.summarize(__data_summary_dfs[0].to_spark())\n      elif type(__data_summary_dfs[0]).__module__ == \"pandas.core.frame\":\n        # pandas dataframe\n        dbutils.data.summarize(spark.createDataFrame(__data_summary_dfs[0]))\n      else:\n        dbutils.data.summarize(__data_summary_dfs[0])\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n  finally:\n    display = __data_summary_display_orig\n    del __data_summary_display_new\n    del __data_summary_display_orig\n    del __data_summary_dfs\n    del __data_summary_user_code_fn\nelse:\n  print(\"This DBR version does not support data profiles.\")",
       "commandTitle": "Data Profile 1",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {},
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "table",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestAssumeRoleInfo": null,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": [
        [
         "ansi",
         874
        ]
       ],
       "metadata": {
        "byteLimit": 2048000,
        "rowLimit": 10000
       },
       "nuid": "c7fa9032-8fe3-4c6d-9c25-9e4a45d944cc",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 4.0,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": null,
       "submitTime": 1759235424753,
       "subtype": "tableResultSubCmd.dataSummary",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "yellowTaxiFilePath = \"abfss://raw@ughnovgs.dfs.core.windows.net/YellowTaxis_202501.csv\"\n",
    "\n",
    "yellowTaxiDF = (\n",
    "    spark.read\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .csv(yellowTaxiFilePath)\n",
    ")\n",
    "\n",
    "yellowTaxiDF.printSchema()\n",
    "\n",
    "display(yellowTaxiDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ddce736-0144-4a94-b1e9-9ce2d7b2282b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "json_example = \"abfss://raw@ughnovgs.dfs.core.windows.net/PaymentTypes.json\"\n",
    "paymentTypes = spark.read.json(json_example)\n",
    "display(paymentTypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7333e3e9-fc01-467a-bfbd-af9f4b160e7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pc = yellowTaxiDF.describe(\"passenger_count\",  \"trip_distance\")\n",
    "display(pc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b669d2f-cb00-45ad-aecf-36e5057e9a3e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Remove zeroes\n",
    "print(f\"Before: {str(yellowTaxiDF.count())}\")\n",
    "yellowTaxiDfRemoveZeroes = yellowTaxiDF.where(yellowTaxiDF.passenger_count > 0).filter(col(\"trip_distance\") > 0)\n",
    "print(f\"After: {str(yellowTaxiDfRemoveZeroes.count())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d59b2854-869b-4a1c-a7f6-faecac6e4ef1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Remove nulls\n",
    "print(f\"Before: {str(yellowTaxiDF.count())}\")\n",
    "yellowTaxiDfDropNulls = yellowTaxiDF.na.drop('all')\n",
    "print(f\"After: {str(yellowTaxiDfDropNulls.count())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1509ffcd-dabe-4a72-8226-3f2921fa1673",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Replace nulls\n",
    "print(f\"Before: {str(yellowTaxiDF.count())}\")\n",
    "yellowTaxidFReplaceNulls = yellowTaxiDF.na.fill('all')\n",
    "print(f\"After: {str(yellowTaxidFReplaceNulls.count())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4fbad36e-5063-4511-aefa-2e81e8ddadde",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Drop duplicates\n",
    "print(f\"Before: {str(yellowTaxiDF.count())}\")\n",
    "\n",
    "yellowTaxiDfNoDuplicates = yellowTaxiDF.dropDuplicates()\n",
    "\n",
    "print(f\"After: {str(yellowTaxiDfNoDuplicates.count())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7beeb64a-9c2f-4677-bbd2-f0e1027fe86c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "# Select only columns we need\n",
    "\n",
    "yellowTaxiFiltered = yellowTaxiDF.select(\n",
    "    'VendorID',\n",
    "    col('passenger_count').cast(IntegerType()),\n",
    "    col('trip_distance').alias('TripDistance'),\n",
    "    col('tpep_dropoff_datetime'),\n",
    "    'tpep_pickup_datetime',\n",
    "    'RatecodeID',\n",
    "    'PULocationID',\n",
    "    'DOLocationID',    \n",
    "    'payment_type',\n",
    "    'fare_amount'\n",
    ")\n",
    "\n",
    "yellowTaxiFiltered.printSchema()\n",
    "display(yellowTaxiFiltered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07048053-8055-4844-9740-51fb9fcf8ae9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Rename columns\n",
    "yellowTaxiRenamed = (\n",
    "    yellowTaxiDF\n",
    "        .withColumnRenamed(\"tpep_pickup_datetime\", \"PickupTime\")\n",
    "        .withColumnRenamed(\"tpep_dropoff_datetime\", \"DropoffTime\")\n",
    "        .withColumnRenamed(\"PULocationID\", \"PickupLocationID\")\n",
    "        .withColumnRenamed(\"DOLocationID\", \"DropoffLocationID\")\n",
    "        .withColumnRenamed(\"total_amount\", \"TotalAmount\")\n",
    "        .withColumnRenamed(\"payment_type\", \"PaymentType\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e80f96b3-a188-40b2-8826-88283ebb1946",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year, month, dayofmonth, col, expr\n",
    "# Derive columns\n",
    "yellowTaxiDerivedFields = (\n",
    "    yellowTaxiRenamed\n",
    "        .withColumn(\"TripYear\", year(col(\"PickupTime\")))\n",
    "        .select(\n",
    "            \"*\", \n",
    "            expr(\"month(PickupTime) AS TripMonth\"),\n",
    "            dayofmonth(col(\"PickupTime\")).alias(\"TripDay\")\n",
    "        )\n",
    ")  \n",
    "\n",
    "display(yellowTaxiDerivedFields)\n",
    "                                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72785ac9-fb9b-4c2c-a713-15e22349ea50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import unix_timestamp, round, col\n",
    "\n",
    "# Add calculated field for trip time in minutes\n",
    "\n",
    "tripTimeInSecondsExpr = unix_timestamp(col(\"DropoffTime\")) - unix_timestamp(col(\"PickupTime\"))\n",
    "                                       \n",
    "tripTimeInMinutesExpr = round(tripTimeInSecondsExpr / 60)\n",
    "\n",
    "yellowTaxiTime = (\n",
    "    yellowTaxiDerivedFields\n",
    "        .withColumn(\"TripTimeInMinutes\", tripTimeInMinutesExpr)\n",
    ")\n",
    "display(yellowTaxiTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "baf3f6e7-3c13-4592-8791-55eebd3e6a17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "# Create derived column - TripType\n",
    "\n",
    "tripTypeColumn = (\n",
    "    when(\n",
    "        col(\"RatecodeID\") == 6,\n",
    "        \"SharedTrip\"\n",
    "    )\n",
    "    .otherwise(\"SoloTrip\")\n",
    ")\n",
    "\n",
    "yellowTaxiTripType = (\n",
    "    yellowTaxiTime\n",
    "        .withColumn(\"TripType\", tripTypeColumn)\n",
    ")\n",
    "display(yellowTaxiTripType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "e0f6d0de-2362-41e3-9e16-ccf466edb5f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, LongType\n",
    "\n",
    "taxiBasesSchema = (\n",
    "                    StructType\n",
    "                    ([\n",
    "                        StructField(\"License Number\"         , StringType()    , True),\n",
    "                        StructField(\"Entity Name\"            , StringType()    , True),\n",
    "                        StructField(\"Telephone Number\"       , LongType()      , True),\n",
    "                        StructField(\"SHL Endorsed\"           , StringType()    , True),\n",
    "                        StructField(\"Type of Base\"           , StringType()    , True),\n",
    "\n",
    "                        StructField(\"Address\", \n",
    "                                        StructType\n",
    "                                        ([\n",
    "                                            StructField(\"Building\"   , StringType(),   True),\n",
    "                                            StructField(\"Street\"     , StringType(),   True), \n",
    "                                            StructField(\"City\"       , StringType(),   True), \n",
    "                                            StructField(\"State\"      , StringType(),   True), \n",
    "                                            StructField(\"Postcode\"   , StringType(),   True)\n",
    "                                        ]),\n",
    "                                    True\n",
    "                                   ),\n",
    "                        \n",
    "                        StructField(\"GeoLocation\", \n",
    "                                        StructType\n",
    "                                        ([\n",
    "                                            StructField(\"Latitude\"   , StringType(),   True),\n",
    "                                            StructField(\"Longitude\"  , StringType(),   True), \n",
    "                                            StructField(\"Location\"   , StringType(),   True)\n",
    "                                        ]),\n",
    "                                    True\n",
    "                                   )  \n",
    "                  ])\n",
    "                )\n",
    "\n",
    "# Read JSON file using the defined schema\n",
    "taxiBasesFilePath = \"abfss://raw@ughnovgs.dfs.core.windows.net/TaxiBases.json\"\n",
    "\n",
    "taxiBasesDF = (\n",
    "                  spark\n",
    "                    .read    \n",
    "                    .option(\"multiline\", \"true\")\n",
    "                    .schema(taxiBasesSchema)\n",
    "                    .json(taxiBasesFilePath)\n",
    "              )\n",
    "\n",
    "display(taxiBasesDF)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "85521378-cc93-4f19-904a-9f8f94ae8468",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Extract nested fields from JSON\n",
    "\n",
    "taxiBasesFlatDF = (\n",
    "\n",
    "                        taxiBasesDF\n",
    "                            .select(\n",
    "                                      col(\"License Number\").alias(\"BaseLicenseNumber\"),\n",
    "                                      col(\"Entity Name\").alias(\"EntityName\"),\n",
    "\n",
    "                                      col(\"Address.Building\").alias(\"AddressBuilding\"),\n",
    "\n",
    "                                      col(\"Address.Street\").alias(\"AddressStreet\"),\n",
    "                                      col(\"Address.City\").alias(\"AddressCity\"),\n",
    "                                      col(\"Address.State\").alias(\"AddressState\"),\n",
    "                                      col(\"Address.Postcode\").alias(\"AddressPostCode\"),\n",
    "\n",
    "                                      col(\"GeoLocation.Latitude\").alias(\"GeoLatitude\"),\n",
    "                                      col(\"GeoLocation.Longitude\").alias(\"GeoLongitude\")\n",
    "                                   )\n",
    "                  )\n",
    "\n",
    "display(taxiBasesFlatDF)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "18aeb6bb-29d7-4c1d-93d3-6c9f637ed725",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg, sum, col\n",
    "\n",
    "# group by and aggregate\n",
    "\n",
    "yellowTaxiDFReport = (\n",
    "    yellowTaxiTime\n",
    "        .groupBy(\"PickupLocationId\", \"DropoffLocationID\")\n",
    "        .agg(\n",
    "            avg(\"TripTimeInMinutes\").alias(\"AvgTripTime\"),\n",
    "            sum(\"TotalAmount\").alias(\"SumAmount\")\n",
    "        )\n",
    "        .orderBy(col(\"PickupLocationId\").desc())\n",
    ")\n",
    "\n",
    "display(yellowTaxiDFReport)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9283c50-6e5b-4560-bce2-c03a363738e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "yellowTaxisParquetOutputPath = \"abfss://raw@ughnovgs.dfs.core.windows.net/Output/YellowTaxis.parquet\"\n",
    "\n",
    "# Write output in parquet format\n",
    "\n",
    "(\n",
    "    yellowTaxiDF\n",
    "        .write\n",
    "        .mode(\"overwrite\")                                  # Other modes: append, errorifexists, ignore\n",
    "        .partitionBy(\"VendorID\")\n",
    "        .format(\"parquet\")                                  # Other formats: csv, json, avro, jdbc, etc.\n",
    "        .save(yellowTaxisParquetOutputPath)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c383fe0-51da-4ac9-9118-9ecbeccfb340",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "yellowTaxisParquetOutputPath = \"abfss://raw@ughnovgs.dfs.core.windows.net/Output/YellowTaxis.delta\"\n",
    "\n",
    "# Write output in delta format\n",
    "\n",
    "(\n",
    "    yellowTaxiDF\n",
    "        .write\n",
    "        .mode(\"overwrite\")                                  # Other modes: append, errorifexists, ignore\n",
    "        .partitionBy(\"VendorID\")\n",
    "        .format(\"delta\")                                  # Other formats: csv, json, avro, jdbc, etc.\n",
    "        .save(yellowTaxisParquetOutputPath)\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Data Prep and Exploration in Databricks",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
